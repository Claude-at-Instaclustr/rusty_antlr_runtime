//! Lexer implementation
use std::borrow::Cow::Borrowed;
use std::borrow::{Borrow, Cow};
use std::cell::{Cell, RefCell};
use std::collections::HashMap;

use std::rc::Rc;

use crate::char_stream::{CharStream, InputData};
use crate::error_listener::{ConsoleErrorListener, ErrorListener};
use crate::errors::{ANTLRError, BaseRecognitionError};
use crate::int_stream::IntStream;
use crate::lexer_atn_simulator::{ILexerATNSimulator, LexerATNSimulator};

use crate::atn::ATN;
use crate::atn_deserializer::ATNDeserializer;
use crate::dfa::DFA;
use crate::recognizer::{ATNInterpreter, ParseInfo, Recognizer, RecognizerImpl, RULE_INDEX_MAP_TYPE, TOKEN_TYPE_MAP_TYPE};
use crate::token::TOKEN_INVALID_TYPE;
use crate::token_factory::{CommonTokenFactory, TokenAware, TokenFactory};
use crate::token_source::TokenSource;
use crate::tree::{NodeImpl, NodeType};
use crate::vocabulary::{VocabularyImpl};
use std::ops::{DerefMut};
use std::sync::{Arc, RwLock};
use crate::dfa_state::DFAState;
use crate::rule_context::RuleContext;

///  Lexer functionality required by `LexerATNSimulator` to work properly
pub trait Lexer<'input>: Recognizer<'input> {
    /// Concrete input stream used by this parser
    type Input: IntStream;
    /// Same as `TokenStream::get_input_stream` but returns concrete type instance
    /// important for proper inlining in hot code of `LexerATNSimulator`
    fn input(&mut self) -> &mut Self::Input;
    /// Sets channel where current token will be pushed
    ///
    /// By default two channels are available:
    ///  - `LEXER_DEFAULT_TOKEN_CHANNEL`
    ///  - `LEXER_HIDDEN`
    fn set_channel(&mut self, v: isize);

    /// Pushes current mode to internal mode stack and sets `m` as current lexer mode
    /// `pop_mode should be used to recover previous mode
    fn push_mode(&mut self, m: usize);

    /// Pops mode from internal mode stack
    fn pop_mode(&mut self) -> Option<usize>;

    /// Sets type of the current token
    /// Called from action to override token that will be emitted by lexer
    fn set_type(&mut self, t: isize);

    /// Sets lexer mode discarding current one
    fn set_mode(&mut self, m: usize);

    /// Used to informs lexer that it should consider next token as a continuation of the current one
    fn more(&mut self);

    /// Tells lexer to completely ignore and not emit current token.
    fn skip(&mut self);

    #[doc(hidden)]
    fn reset(&mut self);

    #[doc(hidden)]
    fn get_interpreter(&self) -> Option<&LexerATNSimulator>;
}

/// **! Usually generated by ANTLR !**
///
/// This trait combines everything that can be used to extend Lexer behavior
pub trait LexerRecog<'a, T: Recognizer<'a>>: Actions<'a, T> + Sized + 'static {
    /// Callback to extend emit behavior
    fn before_emit(_lexer: &mut T) {}
}

/// Default implementation of Lexer
///
/// Public fields in this struct are intended to be used by embedded actions
#[allow(missing_docs)]
pub struct BaseLexer<
    'input,
    //    T: LexerRecog<'input, Self> + 'static,
    Input: CharStream<TF::From>,
    TF: TokenFactory<'input> = CommonTokenFactory,
> {
    recognizer : RecognizerImpl<'input, dyn Lexer<'input>>,
    // constructor values

    /// `LexerATNSimulator` instance of this lexer
   // pub interpreter: Option<Box<LexerATNSimulator>>,
    /// `CharStream` used by this lexer
   // pub input: Option<Input>,
    //    recog: T,
    tree: NodeImpl<'input>,

    //factory: &'input TF,

    //error_listeners: RefCell<Vec<Box<dyn ErrorListener<'input, Self>>>>,

    lexer_state: LexerState,

    current_pos: Rc<LexerPosition>,
    /// Make it `Some` to override token that is currently being generated by lexer
    pub token: Option<TF::Tok>,
    hit_eof: bool,

    mode_stack: Vec<usize>,
    /// Mode lexer is currently in
    pub mode: usize,
    //atn: Arc<ATN>,
    decision_to_DFA: Arc<Vec<RwLock<DFA>>>,
}

#[derive(Debug)]
pub(crate) struct LexerPosition {
    pub(crate) line: Cell<isize>,
    pub(crate) char_position_in_line: Cell<isize>,
}

pub(crate) struct LexerState {
    pub token_type: isize,
    /// Make it `Some` to override text for token that is currently being generated by lexer
    //pub text: Option<<TF::Data as ToOwned>::Owned>,
    pub text: Option<String>,
    pub channel: isize,
    pub start_char_index: isize,
    pub stop_char_index: isize,
    pub start_column: isize,
    pub start_line: isize,
    pub start_char_pos_in_line: isize,
}

impl LexerState {
    pub fn new() -> LexerState {
    LexerState {
    token_type: super::token::TOKEN_INVALID_TYPE,
    text: None,
    channel: super::token::TOKEN_DEFAULT_CHANNEL,
    start_char_index: -1,
    stop_char_index: 0,
    start_column: 0,
    start_line: -1,
    start_char_pos_in_line: -1,
}
}
}

/*impl<'input, Input, TF> Deref for BaseLexer<'input, Input, TF>
where
    T: LexerRecog<'input, Self> + 'static,
    Input: CharStream<TF::From>,
    TF: TokenFactory<'input>,
{
    type Target = T;
    fn deref(&self) -> &Self::Target { &self.recog }
}

impl<'input, Input, TF> DerefMut for BaseLexer<'input, Input, TF>
where
    Input: CharStream<TF::From>,
    TF: TokenFactory<'input>,
{
    fn deref_mut(&mut self) -> &mut Self::Target { &mut self.recog }
}
*/


/// Default lexer mode id
pub const LEXER_DEFAULT_MODE: usize = 0;
/// Special token type to indicate that lexer should continue current token on next iteration
/// see `Lexer::more()`
pub const LEXER_MORE: isize = -2;
/// Special token type to indicate that lexer should not return current token
/// usually used to skip whitespaces and comments
/// see `Lexer::skip()`
pub const LEXER_SKIP: isize = -3;

#[doc(inline)]
pub use super::token::TOKEN_DEFAULT_CHANNEL as LEXER_DEFAULT_TOKEN_CHANNEL;

#[doc(inline)]
pub use super::token::TOKEN_HIDDEN_CHANNEL as LEXER_HIDDEN;

pub(crate) const LEXER_MIN_CHAR_VALUE: isize = 0x0000;
pub(crate) const LEXER_MAX_CHAR_VALUE: isize = 0x10FFFF;

impl<'input, Input, TF> BaseLexer<'input, Input, TF>
where
    Input: CharStream<TF::From>,
    TF: TokenFactory<'input>,
{
    fn emit_token(&mut self, token: TF::Tok) {
        self.token = Some(token);
    }

    fn emit(&mut self) {
        //<T as LexerRecog<Self>>::before_emit(self);
        self.lexer_state.stop_char_index = self.get_char_index() - 1;
        let token = self
            .factory
            .create(Some(self.input.as_mut().unwrap()), &self.lexer_state);
        self.emit_token(token);
    }

    fn emit_eof(&mut self) {
        let eof_state = LexerState {
            token_type: super::int_stream::EOF,
            text: None,
            channel: LEXER_DEFAULT_TOKEN_CHANNEL,
            start_char_index: self.get_char_index(),
            stop_char_index: self.get_char_index() - 1,
            start_column: self.lexer_state.start_column,
            start_line: self.get_line(),
            start_char_pos_in_line: self.get_char_position_in_line(),
        };
        let token = self.factory.create(None::<&mut Input>, &eof_state);
        self.emit_token(token)
    }

    /// Current position in input stream
    pub fn get_char_index(&self) -> isize {
        self.input.as_ref().unwrap().index()
    }

    /// Current token text
    pub fn get_text<'a>(&'a self) -> Cow<'a, TF::Data>
    where
        'input: 'a,
    {
        self.lexer_state
            .text
            .as_ref()
            .map(|it| Borrowed(it.borrow()))
            // .unwrap_or("")
            .unwrap_or_else(|| {
                let text = self.input.as_ref().unwrap().get_text(
                    self.lexer_state.token_start_char_index,
                    self.get_char_index() - 1,
                );
                TF::get_data(text)
            })
    }

    /// Used from lexer actions to override text of the token that will be emitted next
    pub fn set_text(&mut self, _text: <TF::Data as ToOwned>::Owned) {
        self.text = Some(_text);
    }

    // fn get_all_tokens(&mut self) -> Vec<TF::Tok> { unimplemented!() }

    // fn get_char_error_display(&self, _c: char) -> String { unimplemented!() }

    /// Add error listener
    pub fn add_error_listener(&mut self, listener: Box<dyn ErrorListener<'input, Self>>) {
        self.error_listeners.borrow_mut().push(listener);
    }

    /// Remove and drop all error listeners
    pub fn remove_error_listeners(&mut self) {
        self.error_listeners.borrow_mut().clear();
    }

    /// Creates new lexer instance
    pub fn new(
        grammar_file_name: &'static str,
        rule_names: &'static [&'static str],
        channel_names: &'static [&'static str],
        mode_names: &'static [&'static str],
        vocabulary: VocabularyImpl,
        serialized_atn: &'static [&'static str],
        input: Input,
        interpreter: LexerATNSimulator,
        factory: &'input TF,
    ) -> Self {
        let mut recognizer = RecognizerImpl::new(grammar_file_name,rule_names,channel_names,mode_names,vocabulary,serialized_atn);
        recognizer.set_input_stream( input );
        recognizer.set_interpreter(Some(interpreter));
        recognizer.set_token_factory( Some(factory) );
        let atn = recognizer.get_atn();
        let mut dfa: Vec<RwLock<DFA>> = Vec::new();
        let size = atn.decision_to_state.len();
        for i in 0..size {
            dfa.push(RwLock::new(
                DFA::new( atn.clone(), atn.get_decision_state(i), i as isize).into(),
            ))
        }
        let mut lexer = BaseLexer {
            recognizer,
            tree: NodeImpl::new(NodeType::Empty, None),
//            error_listeners: RefCell::new(vec![Box::new(ConsoleErrorListener {})]),
            lexer_state: LexerState::new(),
            current_pos: Rc::new(LexerPosition {
                line: Cell::new(1),
                char_position_in_line: Cell::new(0),
            }),

            token: None,
            hit_eof: false,
            //            token_factory_source_pair: None,
            mode_stack: Vec::new(),
            mode: LEXER_DEFAULT_MODE,
            decision_to_DFA: Arc::new(dfa),
        };
        let pos = lexer.current_pos.clone();
        lexer.interpreter.as_mut().unwrap().current_pos = pos;
        lexer
    }
}

impl<'input, Input, TF> Recognizer for BaseLexer<'input, Input, TF> {
    fn get_rule_names(&self) -> &[&str] {
        self.recognizer.get_rule_names()
    }

    fn get_vocabulary(&self) -> &VocabularyImpl {
        self.recognizer.get_vocabulary()
    }

    fn get_token_type_map(&self) -> &HashMap<&str, isize> {
        self.recognizer.get_token_type_map()
    }

    fn get_rule_index_map(&self) -> &HashMap<&str, usize> {
        self.recognizer.get_rule_index_map()
    }

    fn get_token_type(&self, token_name: &str) -> isize {
        self.recognizer.get_token_type( token_name )
    }

    fn get_serialized_atn(&self) -> &str {
        self.recognizer.get_serialize_atn()
    }

    fn get_grammar_file_name(&self) -> &str {
        self.recognizer.get_grammar_file_name()
    }

    fn get_atn(&self) -> &ATN {
        self.recognizer.get_atn()
    }

    fn get_interpreter(&self) -> &LexerATNSimulator {
        self.recognizer.get_interpreter()
    }

    fn get_parse_info(&self) -> Option<dyn ParseInfo> {
        self.recognizer.get_parse_info()
    }

    fn set_interpreter(&mut self, interpreter: LexerATNSimulator) {
        self.recognizer.set_interpreter(interpreter)
    }

    fn get_error_header(&self, err: BaseRecognitionError) -> String {
        self.recognizer.get_error_header(err)
    }

    fn add_error_listener(&mut self, listener: Box<dyn ErrorListener<'input, dyn Lexer<'input>>>) {
        self.recognizer.add_error_listener( listener )
    }

    fn get_error_listeners(&self) -> Vec<Box<dyn ErrorListener<'input, dyn Lexer<'input>>>> {
        self.recognizer.get_error_listeners()
    }

    fn sempred(&mut self, local_ctxt: Box<dyn RuleContext>, rule_index: isize, action_index: isize) -> bool {
        self.recognizer.sempred(local_ctxt, rule_index,action_index)
    }

    fn precpred(&mut self, local_ctxt: Box<dyn RuleContext>, precedence: isize) -> bool {
        self.recognizer.precpred(local_ctxt, precedence)

    }

    fn action(&mut self, local_ctxt: Box<dyn RuleContext>, rule_index: isize, action_index: isize) {
        self.recognizer.action(local_ctxt, rule_index,action_index)
    }

    fn get_state(&self) -> isize {
        self.recognizer.get_state()
    }

    fn set_state(&mut self, state: isize) {
        self.recognizer.set_state(state)
    }

    fn get_input_stream(&self) -> Option<IntStream> {
        self.recognizer.get_input_stream()
    }

    fn set_input_stream(&self, input: Option<IntStream>) {
        self.recognizer.set_input_stream(input)
    }

    fn get_token_factory(&self) -> Option<TokenFactory> {
        self.recognizer.get_token_factory()
    }

    fn set_token_factory(&self, factory: Option<TokenFactory>) {
        self.recognizer.set_token_factory( factory )
    }

    fn reset(&mut self) {
        self.recognizer.reset()
    }
}

impl<'input, Input, TF> TokenAware<'input> for BaseLexer<'input, Input, TF>
where
    Input: CharStream<TF::From>,
    TF: TokenFactory<'input>,
{
    type TF = TF;
}

impl<'input, Input, TF> TokenSource<'input> for BaseLexer<'input, Input, TF>
where
    Input: CharStream<TF::From>,
    TF: TokenFactory<'input>,
{
    type TF = TF;
    #[inline]
    #[allow(unused_labels)]
    fn next_token(&mut self) -> <Self::TF as TokenFactory<'input>>::Tok {
        assert!(self.input.is_some());

        let _marker = self.input().mark();
        'outer: loop {
            if self.hit_eof {
                self.emit_eof();
                break;
            }
            self.token = None;
            self.lexer_state.channel = LEXER_DEFAULT_TOKEN_CHANNEL;
            self.lexer_state.start_column = self
                .interpreter
                .as_ref()
                .unwrap()
                .get_char_position_in_line();
            self.lexer_state.start_line = self.interpreter.as_ref().unwrap().get_line();
            self.lexer_state.text = None;
            let index = self.input().index();
            self.lexer_state.start_char_index = index;

            'inner: loop {
                self.lexer_state.token_type = TOKEN_INVALID_TYPE;
                // detach from self, to allow self to be passed deeper
                let mut interpreter = self.interpreter.take().unwrap();
                //                    let mut input = self.input.take().unwrap();
                let result = interpreter.match_token(self.mode, self);
                self.interpreter = Some(interpreter);

                let ttype = result.unwrap_or_else(|err| {
                    //                            println!("error, recovering");
                    notify_listeners(&mut self.error_listeners.borrow_mut(), &err, self);
                    self.interpreter
                        .as_mut()
                        .unwrap()
                        .recover(err, self.input.as_mut().unwrap());
                    LEXER_SKIP
                });
                //                    self.input = Some(input)

                if self.input().la(1) == super::int_stream::EOF {
                    self.hit_eof = true;
                }

                if self.lexer_state.token_type == TOKEN_INVALID_TYPE {
                    self.lexer_state.token_type = ttype;
                }

                if self.lexer_state.token_type == LEXER_SKIP {
                    continue 'outer;
                }

                if self.lexer_state.token_type != LEXER_MORE {
                    break;
                }
            }

            if self.token.is_none() {
                self.emit();
                break;
            }
        }
        self.input().release(_marker);
        self.token.take().unwrap()
    }

    fn get_line(&self) -> isize {
        self.current_pos.line.get()
    }

    fn get_char_position_in_line(&self) -> isize {
        self.current_pos.char_position_in_line.get()
    }

    fn get_input_stream(&mut self) -> Option<&mut dyn IntStream> {
        self.recognizer.get_input_stream()
    }

    fn get_source_name(&self) -> String {
        self.input
            .as_ref()
            .map(|it| it.get_source_name())
            .unwrap_or("<none>".to_string())
    }

    //    fn set_token_factory<'c: 'b>(&mut self, f: &'c TokenFactory) {
    //        self.factory = f;
    //    }

    fn get_token_factory(&self) -> &'input TF {
        self.factory
    }
}

#[cold]
#[inline(never)]
fn notify_listeners<'input, T, Input, TF>(
    liseners: &mut Vec<Box<dyn ErrorListener<'input, BaseLexer<'input, Input, TF>>>>,
    e: &ANTLRError,
    lexer: &BaseLexer<'input, Input, TF>,
) where
    T: LexerRecog<'input, BaseLexer<'input, Input, TF>> + 'static,
    Input: CharStream<TF::From>,
    TF: TokenFactory<'input>,
{
    let inner = lexer
        .input
        .as_ref()
        .unwrap()
        .get_text(lexer.token_start_char_index, lexer.get_char_index());
    let text = format!(
        "token recognition error at: '{}'",
        TF::get_data(inner).to_display()
    );
    for listener in liseners.iter_mut() {
        listener.syntax_error(
            lexer,
            None,
            lexer.lexer_state.start_line,
            lexer.lexer_state.start_column,
            &text,
            Some(e),
        )
    }
}

impl<'input, Input, TF> Lexer<'input> for BaseLexer<'input, Input, TF>
    where Input: CharStream<TF::From>, TF: TokenFactory<'input> {


    type Input = dyn CharStream<Cow<'input, str>>;

    fn input(&mut self) -> &mut Self::Input {
        self.input.as_mut().unwrap()
    }

    fn set_channel(&mut self, v: isize) {
        self.channel = v;
    }

    fn push_mode(&mut self, m: usize) {
        self.mode_stack.push(self.mode);
        self.mode = m;
    }

    fn pop_mode(&mut self) -> Option<usize> {
        self.mode_stack.pop().map(|mode| {
            self.mode = mode;
            mode
        })
    }

    fn set_type(&mut self, t: isize) {
        self.token_type = t;
    }

    fn set_mode(&mut self, m: usize) {
        self.mode = m;
    }

    fn more(&mut self) {
        self.set_type(LEXER_MORE)
    }

    fn skip(&mut self) {
        self.set_type(LEXER_SKIP)
    }

    fn reset(&mut self) {
        self.token = None;
        self.hit_eof = false;
        self.lexer_state = LexerState::new();
        self.mode = LEXER_DEFAULT_MODE;
        self.mode_stack.clear();
        self.recognizer.reset();
    }

    fn get_interpreter(&self) -> &Option<ATNInterpreter> {
        self.recognizer.get_interpreter()
    }

}

const LEXER_MIN_DFA_EDGE : usize = 0;
const LEXER_MAX_DFA_EDGE : usize = 127;

struct SimState {
    index : usize,
    line : usize,
    char_pos : usize,
    dfa_state : DFAState,
}




